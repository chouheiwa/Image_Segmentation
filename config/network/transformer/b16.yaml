patches:
  size: [16, 16]
hidden_size: 768
transformer:
  mlp_dim: 3072
  num_heads: 12
  num_layers: 12
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
classifier: seg
representation_size: null
resnet_pretrained_path: null
pretrained_path: 'ViT-B_16'
patch_size: 16
decoder_channels: [256, 128, 64, 16]
n_classes: 2