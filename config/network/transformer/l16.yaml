patches:
  size: [ 16, 16 ]
hidden_size: 1024
transformer:
  mlp_dim: 4096
  num_heads: 16
  num_layers: 24
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
representation_size: null
# custom
classifier: seg
resnet_pretrained_model_name: null
pretrained_model_name: 'ViT-L_16'
decoder_channels: [ 256, 128, 64, 16 ]
n_classes: 2
activation: softmax
